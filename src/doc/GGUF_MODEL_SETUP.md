# ü§ñ Configura√ß√£o do Modelo GGUF para Docker

## üì• Download do Modelo

A aplica√ß√£o est√° configurada para usar o modelo **Llama 3.2 3B Instruct** em formato GGUF.

### Op√ß√£o 1: Download via Hugging Face (Recomendado)

```bash
# Criar pasta models se n√£o existir
mkdir models

# Baixar modelo (escolha um dos links abaixo)
```

**Links para Download:**

1. **Llama 3.2 3B Instruct Q4_K_M** (~2.0 GB)
   - URL: https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf
   - Comando:
   ```bash
   curl -L "https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf" -o models/llama-3.2-3b-instruct.Q4_K_M.gguf
   ```

2. **Llama 3.2 3B Instruct Q5_K_M** (~2.3 GB - Melhor qualidade)
   - URL: https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf

3. **Llama 3.2 3B Instruct Q3_K_M** (~1.5 GB - Mais r√°pido)
   - URL: https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q3_K_M.gguf

### Op√ß√£o 2: Usar outro modelo GGUF

Se voc√™ j√° tem um modelo GGUF, coloque-o na pasta `models/` e atualize o `docker-compose.yml`:

```yaml
environment:
  - GgufModelSettings__ModelPath=/app/models/SEU-MODELO.gguf
```

## üîß Estrutura de Pastas

```
Chat_micro/
‚îú‚îÄ‚îÄ models/                          # ‚Üê Coloque os modelos aqui
‚îÇ   ‚îî‚îÄ‚îÄ llama-3.2-3b-instruct.Q4_K_M.gguf
‚îú‚îÄ‚îÄ docker-compose.yml
‚îî‚îÄ‚îÄ ...
```

## üöÄ Ap√≥s Baixar o Modelo

1. **Rebuild da imagem Docker** (se necess√°rio):
   ```bash
   docker build -t chat-minimal-api:latest .
   ```

2. **Reiniciar containers**:
   ```bash
   docker-compose down
   docker-compose up -d
   ```

3. **Verificar logs**:
   ```bash
   docker logs chat-minimal-api -f
   ```

   Voc√™ deve ver:
   ```
   Carregando modelo GGUF...
   Modelo carregado com sucesso!
   ```

## üß™ Testar a IA

```bash
curl -X POST http://localhost:5120/api/chat/task \
  -H "Content-Type: application/json" \
  -H "X-API-KEY: test-api-key-12345678901234567890123456789012" \
  -d @scripts/test-docker-chat.json
```

## ‚öôÔ∏è Configura√ß√µes Avan√ßadas

### Ajustar Threads e GPU

Edite `appsettings.Production.json`:

```json
"GgufModelSettings": {
  "ModelPath": "/app/models/llama-3.2-3b-instruct.Q4_K_M.gguf",
  "ContextSize": 4096,
  "GpuLayerCount": 0,      // ‚Üê Aumente para usar GPU (se dispon√≠vel)
  "Threads": 4,            // ‚Üê Ajuste conforme CPU
  "Seed": 1337,
  "Verbose": true          // ‚Üê true para debug
}
```

### Usar GPU no Docker

Para usar GPU NVIDIA no Docker:

1. Instale NVIDIA Container Toolkit
2. Atualize `docker-compose.yml`:
   ```yaml
   app:
     deploy:
       resources:
         reservations:
           devices:
             - driver: nvidia
               count: 1
               capabilities: [gpu]
   ```
3. Aumente `GpuLayerCount` no appsettings

## üìä Tamanhos e Performance

| Quantiza√ß√£o | Tamanho | RAM M√≠nima | Velocidade | Qualidade |
|-------------|---------|------------|------------|-----------|
| Q3_K_M      | ~1.5 GB | 3 GB       | ‚ö°‚ö°‚ö°      | ‚≠ê‚≠ê      |
| Q4_K_M      | ~2.0 GB | 4 GB       | ‚ö°‚ö°        | ‚≠ê‚≠ê‚≠ê    |
| Q5_K_M      | ~2.3 GB | 5 GB       | ‚ö°          | ‚≠ê‚≠ê‚≠ê‚≠ê  |
| Q8_0        | ~3.5 GB | 6 GB       | ‚ö°          | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê|

**Recomenda√ß√£o**: Q4_K_M oferece o melhor equil√≠brio entre qualidade e performance.

## ‚ùå Troubleshooting

### Erro: "Modelo GGUF n√£o encontrado"
- Verifique se o arquivo est√° em `models/`
- Confirme o nome do arquivo no `docker-compose.yml`
- Reinicie os containers

### Erro: "Out of memory"
- Use modelo menor (Q3_K_M)
- Reduza `ContextSize`
- Aumente RAM do Docker Desktop

### Modelo carrega mas n√£o responde
- Verifique logs: `docker logs chat-minimal-api`
- Aumente `Threads` se CPU estiver ociosa
- Teste com `Verbose: true`

## üîó Links √öteis

- Hugging Face GGUF Models: https://huggingface.co/models?search=gguf
- LlamaCpp Documentation: https://github.com/ggerganov/llama.cpp
- Quantization Guide: https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/README.md
